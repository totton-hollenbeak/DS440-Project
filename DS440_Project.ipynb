{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medmnist import ChestMNIST\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/thollenbeak/.medmnist/chestmnist_224.npz\n"
     ]
    }
   ],
   "source": [
    "# Load the ChestMNIST dataset\n",
    "dataset = ChestMNIST(split=\"train\", download=True, size=224)\n",
    "\n",
    "n = 1000\n",
    "\n",
    "images = dataset.imgs[0:n]\n",
    "labels = dataset.labels[0:n]\n",
    "\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the image data to ensure the standardization values of the pre-trained model is appropriate\n",
    "# flattened_images = [image.ravel() for image in images]\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# normalized_images = scaler.fit_transform(flattened_images)\n",
    "\n",
    "# images = [image.reshape(224, 224) for image in normalized_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2250.96it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transformed_images = []\n",
    "\n",
    "for image in tqdm.tqdm(images):\n",
    "    image = np.float32(image) / 255.0\n",
    "    image = Image.fromarray(image)\n",
    "    transformed_images.append(preprocess(image))\n",
    "\n",
    "x_train_tensor = torch.stack(transformed_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory required: 0.56 GB\n"
     ]
    }
   ],
   "source": [
    "# Debugging\n",
    "num_images = len(transformed_images)\n",
    "image_size = transformed_images[0].numel()  # Number of elements in one image\n",
    "dtype_size = transformed_images[0].element_size()  # Size of each element in bytes\n",
    "total_memory = num_images * image_size * dtype_size\n",
    "print(f\"Total memory required: {total_memory / (1024 ** 3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(labels)\n",
    "#y_validation_tensor = torch.tensor(labels)\n",
    "#y_test_tensor = torch.tensor(labels)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "#validation_dataset = TensorDataset(x_validation_tensor, y_validation_tensor)\n",
    "#test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders for efficient training and testing data handling\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.squeezenet1_1(weights=models.SqueezeNet1_1_Weights.DEFAULT, progress=True)\n",
    "\n",
    "for param in model.features[:11].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier[1] = nn.Conv2d(512, labels.shape[1], kernel_size=(1, 1), stride=(1, 1))\n",
    "model.classifier[2] = nn.Identity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "scheduler = StepLR(optimizer, step_size = 2, gamma = 0.5)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:12<00:00,  1.24it/s]\n",
      "100%|██████████| 16/16 [00:12<00:00,  1.29it/s]\n",
      "100%|██████████| 16/16 [00:12<00:00,  1.24it/s]\n",
      "100%|██████████| 16/16 [00:12<00:00,  1.31it/s]\n",
      "100%|██████████| 16/16 [00:12<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    \n",
    "    for inputs, targets in tqdm.tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        targets = targets.float()\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Implement validation step later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS440",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
